{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c34a3043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../2.SememeV2/pretrained_model/bert-base-chinese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ../2.SememeV2/pretrained_model/simbert-chinese-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ../2.SememeV2/pretrained_model/chinese-bert-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at ../2.SememeV2/pretrained_model/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 27318/27318 [18:20<00:00, 24.82it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "num = 3\n",
    "\n",
    "def count_avg(output):\n",
    "    sum_all = output[1, :]\n",
    "    for i in list(range(2, num + 1)):\n",
    "        sum_all += output[i, :]\n",
    "    \n",
    "    return sum_all / num\n",
    "\n",
    "df = pd.DataFrame(columns=('Token', 'bert-base-chinese', 'bert-chinese-wwm', 'roberta-chinese-wwm', 'simbert-chinese-base'))\n",
    "\n",
    "input_text = list(map(lambda x: x.strip(), open(f\"./data/ci_{num}_random.txt\", \"r\", encoding=\"utf-8\").readlines()))\n",
    "\n",
    "bert_model_path = \"../2.SememeV2/pretrained_model/bert-base-chinese\"\n",
    "simbert_model_path = \"../2.SememeV2/pretrained_model/simbert-chinese-base\"\n",
    "bert_wwm_model_path = \"../2.SememeV2/pretrained_model/chinese-bert-wwm-ext\"\n",
    "roberta_wwm_model_path = \"../2.SememeV2/pretrained_model/chinese-roberta-wwm-ext\"\n",
    "\n",
    "bert_tkn = AutoTokenizer.from_pretrained(bert_model_path)\n",
    "simbert_tkn = AutoTokenizer.from_pretrained(simbert_model_path)\n",
    "bert_wwm_tkn = AutoTokenizer.from_pretrained(bert_wwm_model_path)\n",
    "roberta_wwm_tkn = AutoTokenizer.from_pretrained(roberta_wwm_model_path)\n",
    "\n",
    "bert = AutoModel.from_pretrained(bert_model_path).to(\"cuda:1\")\n",
    "simbert = AutoModel.from_pretrained(simbert_model_path).to(\"cuda:1\")\n",
    "bert_wwm = AutoModel.from_pretrained(bert_wwm_model_path).to(\"cuda:1\")\n",
    "roberta_wwm = AutoModel.from_pretrained(roberta_wwm_model_path).to(\"cuda:1\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for char in tqdm(input_text):\n",
    "\n",
    "        bert_input_encoded = { k: v.to(\"cuda:1\") for k,v in bert_tkn(char, return_tensors=\"pt\").items() }\n",
    "        simbert_input_encoded = { k: v.to(\"cuda:1\") for k,v in simbert_tkn(char, return_tensors=\"pt\").items() }\n",
    "        bert_wwm_input_encoded = { k: v.to(\"cuda:1\") for k,v in bert_wwm_tkn(char, return_tensors=\"pt\").items() }\n",
    "        roberta_wwm_input_encoded = { k: v.to(\"cuda:1\") for k,v in roberta_wwm_tkn(char, return_tensors=\"pt\").items() }\n",
    "\n",
    "        bert_out = bert(**bert_input_encoded).last_hidden_state.squeeze(0)\n",
    "        bert_char_similarity = torch.cosine_similarity(\n",
    "            count_avg(bert_out), bert_out[0, :],\n",
    "            dim=0, eps=1e-08\n",
    "        ).item()\n",
    "\n",
    "        simbert_out = simbert(**simbert_input_encoded).last_hidden_state.squeeze(0)\n",
    "        simbert_char_similarity = torch.cosine_similarity(\n",
    "            count_avg(simbert_out), simbert_out[0, :],\n",
    "            dim=0, eps=1e-08\n",
    "        ).item()\n",
    "\n",
    "        bert_wwm_out = bert_wwm(**bert_wwm_input_encoded).last_hidden_state.squeeze(0)\n",
    "        bert_wwm_char_similarity = torch.cosine_similarity(\n",
    "            count_avg(bert_wwm_out), bert_wwm_out[0, :],\n",
    "            dim=0, eps=1e-08\n",
    "        ).item()\n",
    "\n",
    "        roberta_wwm_out = roberta_wwm(**roberta_wwm_input_encoded).last_hidden_state.squeeze(0)\n",
    "        roberta_wwm_char_similarity = torch.cosine_similarity(\n",
    "            count_avg(roberta_wwm_out), roberta_wwm_out[0, :],\n",
    "            dim=0, eps=1e-08\n",
    "        ).item()\n",
    "\n",
    "        df = df.append(pd.Series({\n",
    "            'Token':char,\n",
    "            'bert-base-chinese':bert_char_similarity,\n",
    "            'bert-chinese-wwm':bert_wwm_char_similarity,\n",
    "            'roberta-chinese-wwm':roberta_wwm_char_similarity,\n",
    "            'simbert-chinese-base':simbert_char_similarity\n",
    "        }), ignore_index=True)\n",
    "\n",
    "df.to_excel(f\"./CLS与avg(TOK[{num}])_random.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
